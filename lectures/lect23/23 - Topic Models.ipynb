{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 23: Topic Model with LDA\n",
    "\n",
    "Excerpt from: http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/ (One of the easiest explanations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Suppose you have the following set of sentences:\n",
    "\n",
    "- I like to eat broccoli and bananas.\n",
    "- I ate a banana and spinach smoothie for breakfast.\n",
    "- Chinchillas and kittens are cute.\n",
    "- My sister adopted a kitten yesterday.\n",
    "- Look at this cute hamster munching on a piece of broccoli.\n",
    "\n",
    "What is latent Dirichlet allocation? It’s a way of automatically discovering *topics* that these sentences contain. For example, given these sentences and asked for 2 topics, LDA might produce something like\n",
    "\n",
    "- **Sentences 1 and 2**: 100% Topic A\n",
    "- **Sentences 3 and 4**: 100% Topic B\n",
    "- **Sentence 5**: 60% Topic A, 40% Topic B\n",
    "- **Topic A**: 30% broccoli, 15% bananas, 10% breakfast, 10% munching, … (at which point, you could interpret topic A to be about food)\n",
    "- **Topic B**: 20% chinchillas, 20% kittens, 20% cute, 15% hamster, … (at which point, you could interpret topic B to be about cute animals)\n",
    "\n",
    "The question, of course, is: how does LDA perform this discovery?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model\n",
    "\n",
    "In more detail, LDA represents documents as **mixtures of topics** that spit out words with certain probabilities. It assumes that documents are produced in the following fashion: when writing each document, you\n",
    "\n",
    "- Decide on the number of words N the document will have (say, according to a Poisson distribution).\n",
    "- Choose a topic mixture for the document (according to a Dirichlet distribution over a fixed set of K topics). For example, assuming that we have the two food and cute animal topics above, you might choose the document to consist of 1/3 food and 2/3 cute animals.\n",
    "- Generate each word w_i in the document by:\n",
    "  - First picking a topic (according to the multinomial distribution that you sampled above; for example, you might pick the food topic with 1/3 probability and the cute animals topic with 2/3 probability).\n",
    "  - Using the topic to generate the word itself (according to the topic’s multinomial distribution). For example, if we selected the food topic, we might generate the word “broccoli” with 30% probability, “bananas” with 15% probability, and so on.\n",
    "  \n",
    "Assuming this generative model for a collection of documents, LDA then tries to backtrack from the documents to find a set of topics that are likely to have generated the collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Let’s make an example. According to the above process, when generating some particular document D, you might\n",
    "\n",
    "- Pick 5 to be the number of words in D.\n",
    "- Decide that D will be 1/2 about food and 1/2 about cute animals.\n",
    "- Pick the first word to come from the food topic, which then gives you the word “broccoli”.\n",
    "- Pick the second word to come from the cute animals topic, which gives you “panda”.\n",
    "- Pick the third word to come from the cute animals topic, giving you “adorable”.\n",
    "- Pick the fourth word to come from the food topic, giving you “cherries”.\n",
    "- Pick the fifth word to come from the food topic, giving you “eating”.\n",
    "\n",
    "So the document generated under the LDA model will be “broccoli panda adorable cherries eating” (note that LDA is a bag-of-words model).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "So now suppose you have a set of documents. You’ve chosen some fixed number of K topics to discover, and want to use LDA to learn the topic representation of each document and the words associated to each topic. How do you do this? One way (known as collapsed Gibbs sampling) is the following:\n",
    "\n",
    "- Go through each document, and randomly assign each word in the document to one of the K topics.\n",
    "- Notice that this random assignment already gives you both topic representations of all the documents and word distributions of all the topics (albeit not very good ones).\n",
    "- So to improve on them, for each document d…\n",
    "  - Go through each word w in d…\n",
    "    - And for each topic t, compute two things: 1) p(topic t | document d) = the proportion of words in document d that are currently assigned to topic t, and 2) p(word w | topic t) = the proportion of assignments to topic t over all documents that come from this word w. Reassign w a new topic, where we choose topic t with probability p(topic t | document d) * p(word w | topic t) (according to our generative model, this is essentially the probability that topic t generated word w, so it makes sense that we resample the current word’s topic with this probability). (Also, I’m glossing over a couple of things here, in particular the use of priors/pseudocounts in these probabilities.)\n",
    "    - In other words, in this step, we’re assuming that all topic assignments except for the current word in question are correct, and then updating the assignment of the current word using our model of how documents are generated.\n",
    "- After repeating the previous step a large number of times, you’ll eventually reach a roughly steady state where your assignments are pretty good. So use these assignments to estimate the topic mixtures of each document (by counting the proportion of words assigned to each topic within that document) and the words associated to each topic (by counting the proportion of words assigned to each topic overall)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "    \n",
    "# create sample documents\n",
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'brocolli', u'good', u'eat', u'brother', u'like', u'eat', u'good', u'brocolli', u'mother'], [u'mother', u'spend', u'lot', u'time', u'drive', u'brother', u'around', u'basebal', u'practic'], [u'health', u'expert', u'suggest', u'drive', u'may', u'caus', u'increas', u'tension', u'blood', u'pressur'], [u'often', u'feel', u'pressur', u'perform', u'well', u'school', u'mother', u'never', u'seem', u'drive', u'brother', u'better'], [u'health', u'profession', u'say', u'brocolli', u'good', u'health']]\n",
      "[(0, 2), (1, 2), (2, 1), (3, 1), (4, 1), (5, 2)]\n",
      "[(3, 1), (4, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)]\n"
     ]
    }
   ],
   "source": [
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "    \n",
    "print texts\n",
    "    \n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "print corpus[0]\n",
    "print corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.068*\"mother\" + 0.068*\"brother\" + 0.068*\"drive\" + 0.041*\"pressur\" + 0.040*\"often\"'), (1, u'0.086*\"good\" + 0.086*\"health\" + 0.086*\"brocolli\" + 0.061*\"eat\" + 0.037*\"may\"')]\n"
     ]
    }
   ],
   "source": [
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)\n",
    "print (ldamodel.print_topics(num_topics=2, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.031*\"brocolli\" + 0.031*\"good\" + 0.031*\"drive\" + 0.031*\"pressur\" + 0.031*\"health\"'), (1, u'0.082*\"brother\" + 0.082*\"mother\" + 0.058*\"brocolli\" + 0.058*\"good\" + 0.057*\"drive\"'), (2, u'0.125*\"health\" + 0.050*\"pressur\" + 0.050*\"drive\" + 0.050*\"increas\" + 0.050*\"suggest\"')]\n"
     ]
    }
   ],
   "source": [
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)\n",
    "print (ldamodel.print_topics(num_topics=3, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  u'0.082*\"brother\" + 0.082*\"mother\" + 0.058*\"brocolli\" + 0.058*\"good\" + 0.057*\"drive\"'),\n",
       " (0,\n",
       "  u'0.031*\"brocolli\" + 0.031*\"good\" + 0.031*\"drive\" + 0.031*\"pressur\" + 0.031*\"health\"')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.show_topics(num_topics=2, num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
